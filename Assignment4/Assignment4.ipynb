{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ace_tools_open as tools\n",
    "from case_studies import *\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save case study functions, their derivatives and hessians in lists\n",
    "fs = [f1, f4]\n",
    "dfs = [df1, df4]\n",
    "Hfs = [Hf1, Hf4]\n",
    "fnames = [\"f1\", \"f4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, df, x, pk, alpha_init, c1, rho):\n",
    "    \n",
    "    alpha = alpha_init\n",
    "    while f(x + alpha * pk) > f(x) + c1 * alpha * np.dot(df(x), pk):\n",
    "        alpha *= rho\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max iter afhænger nok af newton som kalder CG, måske skal det hedde noget andet\n",
    "def conjugate_gradients(Q, g, eps, max_iter=1000):\n",
    "    x = 0\n",
    "    grad = g\n",
    "    p = -grad\n",
    "    counter = 0\n",
    "    for i in range(max_iter):\n",
    "        Qp = np.dot(Q, p)\n",
    "        alpha_k = -(np.dot(p, grad) / np.dot(p, Qp))\n",
    "\n",
    "        x = x + alpha_k * p\n",
    "        \n",
    "        grad = np.dot(Q, x) + g\n",
    "        \n",
    "        if np.linalg.norm(grad) < eps:\n",
    "            break\n",
    "\n",
    "        p = -grad + (np.dot(grad, Qp) / np.dot(p, Qp)) * p\n",
    "        grad = grad\n",
    "        counter += 1\n",
    "    return x, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_newton(x, f, df, hf, c1=1e-4, rho=0.9, max_iter=1000, tol=1e-6):\n",
    "    xs = [x]\n",
    "    cg_count_per_iter = []\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        if np.linalg.norm(df(x)) < tol:\n",
    "            cg_count_per_iter.append(0)\n",
    "            break \n",
    "        \n",
    "        n_k = min(0.5, np.linalg.norm(df(x)))\n",
    "        #n_k = 0.5 * min(0.5, np.sqrt(np.linalg.norm(df(x))))\n",
    "        eps = n_k * np.linalg.norm(df(x))\n",
    "        p, counter = conjugate_gradients(hf(x), df(x), eps)\n",
    "        alpha = backtracking_line_search(f, df, x, p, 1.0, c1, rho)\n",
    "        x = x + alpha * p\n",
    "        xs.append(x)\n",
    "\n",
    "        cg_count_per_iter.append(counter)\n",
    "    return x, xs, cg_count_per_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(x0, f, grad_f, hessian_f, c1=1e-4, rho=0.9, tol=1e-6, max_iters=1000):\n",
    "    x = x0\n",
    "    xs = [x0]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        grad = grad_f(x)\n",
    "        hessian = hessian_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break \n",
    "        \n",
    "        if np.all(np.linalg.eigvals(hessian) > 0):\n",
    "            p = -np.linalg.solve(hessian, grad)\n",
    "        else:\n",
    "            eigvals, eigvecs = np.linalg.eigh(hessian)\n",
    "            H = sum((1 / abs(eigval)) * np.outer(eigvec, eigvec) for eigval, eigvec in zip(eigvals, eigvecs))\n",
    "            p = -H @ grad\n",
    "        \n",
    "        alpha = backtracking_line_search(f, grad_f, x, p, 1.0, c1, rho)\n",
    "        x = x + alpha * p\n",
    "        xs.append(x)\n",
    "    \n",
    "    return x, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scipy_bfgs(f,df,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"BFGS\", jac=df, tol=epsilon,options={'maxiter':max_iterations, 'gtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strict Wolfe condition Linesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe_search(f, df, x, p, c1, c2, alpha_init):\n",
    "    def gf(alpha):\n",
    "        return f(x + np.dot(alpha, p))\n",
    "    def g_prime(alpha):\n",
    "        return np.dot(df(x + alpha * p), p)\n",
    "    l = 0\n",
    "    u = alpha_init\n",
    "    while True:\n",
    "        if gf(u) > (gf(0) + c1 * u * g_prime(0)) or gf(u) > gf(l):\n",
    "            break\n",
    "        if abs(g_prime(u)) < c2 * abs(g_prime(0)):\n",
    "            alpha_star = u\n",
    "            return alpha_star\n",
    "        if g_prime(u) > 0:\n",
    "            break\n",
    "        else:\n",
    "            u = u * 2\n",
    "    while True:\n",
    "        a = (l + u) / 2\n",
    "        if gf(a) > (gf(0) + c1 * a * g_prime(0)) or gf(a) > gf(l):\n",
    "            u = a\n",
    "        else:\n",
    "            if abs(g_prime(a)) < c2 * abs(g_prime(0)):\n",
    "                alpha_star = a\n",
    "                return alpha_star\n",
    "            if g_prime(a) < 0:\n",
    "                l = a\n",
    "            else:\n",
    "                u = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x, f, df, c1=1e-4, c2=0.3, max_iter=1000, tol=1e-6):\n",
    "    # eye tager dimension\n",
    "    H = np.eye(20)\n",
    "    for _ in range(max_iter):\n",
    "        p = -np.dot(H, df(x))\n",
    "        alpha = wolfe_search(f, df, x, p, 1.0, c1, c2)\n",
    "        ap = np.dot(alpha, p)\n",
    "        x_new = x + ap\n",
    "        y = df(x_new) - df(x)\n",
    "        s = ap\n",
    "        H = H + (np.dot(s, y) + np.dot(y, np.dot(H, y))) / (np.dot(s, y))**2 * np.dot(s, s) - ((np.dot(H, np.dot(y, s)) + np.dot(s, np.dot(y, H))) / (np.dot(s, y)))\n",
    "        x = x_new\n",
    "        if np.linalg.norm(df(x)) < tol:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, df, optimizer, x0, x_opt, Hf=None):\n",
    "    start_time = time.time()\n",
    "    if optimizer == bfgs:\n",
    "        x_final, xs = optimizer(x0, f, df)\n",
    "    if optimizer == newtons_method:\n",
    "        x_final, xs = optimizer(x0, f, df, Hf)\n",
    "    if optimizer == approximate_newton:\n",
    "        x_final, xs, cg_count_per_iter = optimizer(x0, f, df, Hf)\n",
    "    # steepest skal ikke have HF\n",
    "    end_time = time.time()\n",
    "\n",
    "    num_iterations = len(xs)\n",
    "    final_solution_point = x_final\n",
    "    dist_to_optimum = np.linalg.norm(x_final-x_opt)\n",
    "    final_fun_value = f(xs[-1])\n",
    "    duration = end_time-start_time\n",
    "    \n",
    "    grad_norms = [np.linalg.norm(df(x)) for x in xs]\n",
    "    if optimizer == approximate_newton:\n",
    "        return (num_iterations, duration, final_fun_value, final_solution_point, dist_to_optimum, grad_norms, grad_norms[-1], cg_count_per_iter)\n",
    "\n",
    "    return (num_iterations, duration, final_fun_value, final_solution_point, dist_to_optimum, grad_norms, grad_norms[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10\n",
    "np.random.seed(SEED)\n",
    "\n",
    "d = 20\n",
    "num_runs = 3\n",
    "\n",
    "benchmark_results = []\n",
    "\n",
    "\n",
    "for f, df, Hf, fname in zip(fs, dfs, Hfs, fnames):\n",
    "    x_optimal = x_opt(f, d)\n",
    "    #Approx Newtons Method\n",
    "    for _ in range(num_runs):\n",
    "        x0 = np.random.randn(d)        \n",
    "        sd_result = benchmark(f, df, approximate_newton, x0, x_optimal, Hf)\n",
    "        benchmark_results.append((fname, \"Approximate Newton\") + sd_result)\n",
    "\n",
    "    #Original Newtons Method\n",
    "    for _ in range(num_runs):\n",
    "        x0 = np.random.randn(d)  \n",
    "        sd_result = benchmark(f, df, newtons_method, x0, x_optimal, Hf)\n",
    "        benchmark_results.append((fname, \"Newtons Method\") + sd_result)\n",
    "\n",
    "    #BFGS\n",
    "    for _ in range(num_runs):\n",
    "        x0 = np.random.randn(d)\n",
    "        sd_result = benchmark(f, df, bfgs, x0, x_optimal)\n",
    "        benchmark_results.append((fname, \"BFGS\") + sd_result)\n",
    "# Convert to DataFrame\n",
    "columns = [\"Function\", \"Optimizer\", \"Iterations\", \"Time\", \"Final Function Value\", \"Final Solution Point\", \"Distance to Optimum\", \"Gradient Norms\", \"Final Gradient Norm\", \"CG iterations per Newton iteration\"]\n",
    "df_results = pd.DataFrame(benchmark_results, columns=columns)\n",
    "\n",
    "df_display = df_results.drop(columns=[\"Final Solution Point\", \"Gradient Norms\", \"CG iterations per Newton iteration\"])\n",
    "tools.display_dataframe_to_user(name=\"Benchmark Results\", dataframe=df_display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
