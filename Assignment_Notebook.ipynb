{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbca1368",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'case_studies'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#if this fails, you need to put the case_studies.py file in the same folder\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcase_studies\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'case_studies'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#if this fails, you need to put the case_studies.py file in the same folder\n",
    "from case_studies import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825955c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#These are the example optimizers you should evaluate this week.\n",
    "#These ar eoptimizers implemented in scipy.\n",
    "#they take as first 2 or 3 arguments the function f, its gradient df and sometimes its hessian Hf.\n",
    "#the next parameters are all the same: x0 is the starting point, max_iterations the stopping criterion for iterations\n",
    "#and epsilon the precision tolerance to be reached. \n",
    "#Note: epsilon is interpreted slightly differently across algorithms, and some algorithms might not reach the tolerance\n",
    "#and stop early.\n",
    "def scipy_bfgs(f,df,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"BFGS\", jac=df, tol=epsilon,options={'maxiter':max_iterations, 'gtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)\n",
    "\n",
    "def scipy_newton(f,df,Hf,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"Newton-CG\", jac=df, hess=Hf, tol=epsilon,options={'maxiter':max_iterations,'xtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)\n",
    "\n",
    "def scipy_trust_region(f,df,Hf,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"trust-exact\", jac=df, hess=Hf, tol=epsilon,options={'maxiter':max_iterations})\n",
    "    return np.array(xs), np.array(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage of the algorithms\n",
    "#the output is a list of points evaluated on the function as well as the gradient norms at that point\n",
    "#this algorithms has the first three arguments functions for function value, gradient and Hessian.\n",
    "#For the 5 functions, those are named f1-f5 etc and cna be found in the case_studies.py file\n",
    "x0=np.ones(2)\n",
    "xs,grad_norms = scipy_trust_region(f4,df4,Hf4,x0, 1000, 1.e-10)\n",
    "\n",
    "\n",
    "#the optimal point for a given function and dimensionality is stored in the package as well for at least 15 decimals precision\n",
    "optimal = x_opt(\"f4\", 2)\n",
    "print(\"final solution point:\", xs[-1])\n",
    "print(\"distance of x from optimum\", np.linalg.norm(xs[-1]-optimal))\n",
    "print(\"number of function evaluations:\", len(grad_norms))\n",
    "print(\"final function value:\", f4(xs[-1]))\n",
    "print(\"final gradient norm:\", grad_norms[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NumOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
