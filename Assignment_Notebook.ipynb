{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbca1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#if this fails, you need to put the case_studies.py file in the same folder\n",
    "from case_studies import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825955c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#These are the example optimizers you should evaluate this week.\n",
    "#These ar eoptimizers implemented in scipy.\n",
    "#they take as first 2 or 3 arguments the function f, its gradient df and sometimes its hessian Hf.\n",
    "#the next parameters are all the same: x0 is the starting point, max_iterations the stopping criterion for iterations\n",
    "#and epsilon the precision tolerance to be reached. \n",
    "#Note: epsilon is interpreted slightly differently across algorithms, and some algorithms might not reach the tolerance\n",
    "#and stop early.\n",
    "def scipy_bfgs(f,df,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"BFGS\", jac=df, tol=epsilon,options={'maxiter':max_iterations, 'gtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)\n",
    "\n",
    "def scipy_newton(f,df,Hf,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"Newton-CG\", jac=df, hess=Hf, tol=epsilon,options={'maxiter':max_iterations,'xtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)\n",
    "\n",
    "def scipy_trust_region(f,df,Hf,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"trust-exact\", jac=df, hess=Hf, tol=epsilon,options={'maxiter':max_iterations})\n",
    "    return np.array(xs), np.array(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final solution point: [0.00190933 0.00190933]\n",
      "distance of x from optimum 1.1857524696459338e-11\n",
      "number of function evaluations: 9\n",
      "final function value: 1.2203194789920598e-05\n",
      "final gradient norm: 9.977761920841575e-11\n",
      "Name: f4\n"
     ]
    }
   ],
   "source": [
    "#example usage of the algorithms\n",
    "#the output is a list of points evaluated on the function as well as the gradient norms at that point\n",
    "#this algorithms has the first three arguments functions for function value, gradient and Hessian.\n",
    "#For the 5 functions, those are named f1-f5 etc and cna be found in the case_studies.py file\n",
    "x0=np.ones(2)\n",
    "xs,grad_norms = scipy_trust_region(f4,df4,Hf4,x0, 1000, 1.e-10)\n",
    "\n",
    "\n",
    "#the optimal point for a given function and dimensionality is stored in the package as well for at least 15 decimals precision\n",
    "optimal = x_opt(\"f4\", 2)\n",
    "print(\"final solution point:\", xs[-1])\n",
    "print(\"distance of x from optimum\", np.linalg.norm(xs[-1]-optimal))\n",
    "print(\"number of function evaluations:\", len(grad_norms))\n",
    "print(\"final function value:\", f4(xs[-1]))\n",
    "print(\"final gradient norm:\", grad_norms[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "218765eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function\n",
    "def evaluate_optimizer(f, df, optimizer, x0, max_iterations, eps, Hf=None):\n",
    "    if Hf is not None:\n",
    "        xs, grad_norms = optimizer(f, df, Hf, x0, max_iterations, eps)\n",
    "    else:\n",
    "        xs, grad_norms = optimizer(f, df, x0, max_iterations, eps)\n",
    "    \n",
    "    optimizer_name = optimizer.__name__\n",
    "    fname = f.__name__\n",
    "\n",
    "    optimal_solution = x_opt(fname, len(x0))\n",
    "    final_solution_point = xs[-1]\n",
    "    dist_to_optimum = np.linalg.norm(xs[-1]-optimal_solution)\n",
    "    num_of_fun_evals = len(grad_norms)\n",
    "    final_fun_value = f(xs[-1])\n",
    "    final_grad_norm = grad_norms[-1]\n",
    "    \n",
    "    print(f\"Function: {fname}, Optimizer: {optimizer_name}\")\n",
    "    print( \n",
    "    f\"\"\"Final Solution Point: {final_solution_point}\n",
    "    Distance of x to optimum: {dist_to_optimum}\n",
    "    Number of Function Evaluations: {num_of_fun_evals}\n",
    "    Final Function Value: {final_fun_value}\n",
    "    Final Gradient Norm: {final_grad_norm}\n",
    "    \"\"\")\n",
    "\n",
    "    return {\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"function\": fname,\n",
    "        \"distance_to_optimum\": dist_to_optimum,\n",
    "        \"num_evaluations\": num_of_fun_evals,\n",
    "        \"final_function_value\": final_fun_value,\n",
    "        \"final_gradient_norm\": final_grad_norm,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d655c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: f1, Optimizer: scipy_bfgs\n",
      "Final Solution Point: [1.08420217e-19 0.00000000e+00]\n",
      "    Distance of x to optimum: 1.0842021724855044e-19\n",
      "    Number of Function Evaluations: 6\n",
      "    Final Function Value: 1.1754943508222875e-38\n",
      "    Final Gradient Norm: 1e-15\n",
      "    \n",
      "Function: f1, Optimizer: scipy_newton\n",
      "Final Solution Point: [0. 0.]\n",
      "    Distance of x to optimum: 0.0\n",
      "    Number of Function Evaluations: 3\n",
      "    Final Function Value: 0.0\n",
      "    Final Gradient Norm: 1e-15\n",
      "    \n",
      "Function: f1, Optimizer: scipy_trust_region\n",
      "Final Solution Point: [ 2.22044605e-16 -1.73472348e-18]\n",
      "    Distance of x to optimum: 2.220513810852149e-16\n",
      "    Number of Function Evaluations: 3\n",
      "    Final Function Value: 5.231307211441829e-32\n",
      "    Final Gradient Norm: 3.497753190081524e-15\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Ellipsoid function evaluation\n",
    "x0 = np.ones(2)\n",
    "max_iterations = 1000\n",
    "eps = 1.e-10\n",
    "\n",
    "f1_bfgs = evaluate_optimizer(f1, df1, scipy_bfgs, x0, max_iterations, eps)\n",
    "f1_newton = evaluate_optimizer(f1, df1, scipy_newton, x0, max_iterations, eps, Hf1)\n",
    "f1_trust_region= evaluate_optimizer(f1, df1, scipy_trust_region, x0, max_iterations, eps, Hf1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NumOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
