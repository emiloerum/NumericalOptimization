{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#if this fails, you need to put the case_studies.py file in the same folder\n",
    "from case_studies import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825955c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#These are the example optimizers you should evaluate this week.\n",
    "#These ar eoptimizers implemented in scipy.\n",
    "#they take as first 2 or 3 arguments the function f, its gradient df and sometimes its hessian Hf.\n",
    "#the next parameters are all the same: x0 is the starting point, max_iterations the stopping criterion for iterations\n",
    "#and epsilon the precision tolerance to be reached. \n",
    "#Note: epsilon is interpreted slightly differently across algorithms, and some algorithms might not reach the tolerance\n",
    "#and stop early.\n",
    "def scipy_bfgs(f,df,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"BFGS\", jac=df, tol=epsilon,options={'maxiter':max_iterations, 'gtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)\n",
    "\n",
    "def scipy_newton(f,df,Hf,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"Newton-CG\", jac=df, hess=Hf, tol=epsilon,options={'maxiter':max_iterations,'xtol':epsilon})\n",
    "    return np.array(xs), np.array(grad_norms)\n",
    "\n",
    "def scipy_trust_region(f,df,Hf,x0,max_iterations,epsilon):\n",
    "    xs=[]\n",
    "    grad_norms=[]\n",
    "    def logging_f(x):\n",
    "        xs.append(x)\n",
    "        grad_norms.append(np.maximum(np.linalg.norm(df(x)),10**(-5)*epsilon))\n",
    "        return f(x)\n",
    "    minimize(logging_f, x0, method=\"trust-exact\", jac=df, hess=Hf, tol=epsilon,options={'maxiter':max_iterations})\n",
    "    return np.array(xs), np.array(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage of the algorithms\n",
    "#the output is a list of points evaluated on the function as well as the gradient norms at that point\n",
    "#this algorithms has the first three arguments functions for function value, gradient and Hessian.\n",
    "#For the 5 functions, those are named f1-f5 etc and cna be found in the case_studies.py file\n",
    "x0=np.ones(2)\n",
    "xs,grad_norms = scipy_trust_region(f4,df4,Hf4,x0, 1000, 1.e-10)\n",
    "\n",
    "\n",
    "#the optimal point for a given function and dimensionality is stored in the package as well for at least 15 decimals precision\n",
    "optimal = x_opt(\"f4\", 2)\n",
    "print(\"final solution point:\", xs[-1])\n",
    "print(\"distance of x from optimum\", np.linalg.norm(xs[-1]-optimal))\n",
    "print(\"number of function evaluations:\", len(grad_norms))\n",
    "print(\"final function value:\", f4(xs[-1]))\n",
    "print(\"final gradient norm:\", grad_norms[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c1a89",
   "metadata": {},
   "source": [
    "# Evaluate optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218765eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Evaluation function\n",
    "def evaluate_optimizer(f, df, optimizer, x0, max_iterations, eps, Hf=None):\n",
    "    if Hf is not None:\n",
    "        xs, grad_norms = optimizer(f, df, Hf, x0, max_iterations, eps)\n",
    "    else:\n",
    "        xs, grad_norms = optimizer(f, df, x0, max_iterations, eps)\n",
    "    optimizer_name = optimizer.__name__\n",
    "    fname = f.__name__\n",
    "\n",
    "    optimal_solution = x_opt(fname, len(x0))\n",
    "    final_solution_point = xs[-1]\n",
    "    dist_to_optimum = np.linalg.norm(xs[-1]-optimal_solution)\n",
    "    num_of_fun_evals = len(grad_norms)\n",
    "    final_fun_value = f(xs[-1])\n",
    "    final_grad_norm = grad_norms[-1]\n",
    "\n",
    "    return {\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"function\": fname,\n",
    "        \"distance_to_optimum\": dist_to_optimum,\n",
    "        \"num_evaluations\": num_of_fun_evals,\n",
    "        \"final_function_value\": final_fun_value,\n",
    "        \"final_gradient_norm\": final_grad_norm,\n",
    "        \"grad_norms\": grad_norms\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2695f",
   "metadata": {},
   "source": [
    "# Averages - pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d655c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 1000\n",
    "eps = 1.e-10\n",
    "\n",
    "f_list = [f1, f2, f3, f4, f5]\n",
    "df_list = [df1, df2, df3, df4, df5]\n",
    "hf_list = [Hf1, Hf2, Hf3, Hf4, Hf5]\n",
    "\n",
    "# x0s = [np.array([-10000, -10000]), np.array([-10000, 10000]), np.array([10000, -10000]), np.array([10000, 10000]),\n",
    "                # np.array([-5000, -5000]), np.array([-5000, 5000]), np.array([5000, -5000]), np.array([5000, 5000]),\n",
    "                # np.array([-50, -50]), np.array([-50, 50]), np.array([50, -50]), np.array([50, 50])]\n",
    "\n",
    "x0s = [np.array([-5, -5]), np.array([-5, 5]), np.array([5, -5]), np.array([5, 5]),\n",
    "                np.array([-2, -2]), np.array([-2, 2]), np.array([2, -2]), np.array([2, 2]),\n",
    "                np.array([-1, -1]), np.array([-1, 1]), np.array([1, -1]), np.array([1, 1])] \n",
    "\n",
    "\n",
    "\n",
    "bfgs_results = []\n",
    "newton_results = []\n",
    "trust_region_results = []\n",
    "\n",
    "for f, df, hf in zip(f_list, df_list, hf_list):\n",
    "    for x0 in x0s:\n",
    "        result_bfgs = evaluate_optimizer(f, df, scipy_bfgs, x0, max_iterations, eps)\n",
    "        result_newton = evaluate_optimizer(f, df, scipy_newton, x0, max_iterations, eps, hf)\n",
    "        result_trust_region = evaluate_optimizer(f, df, scipy_trust_region, x0, max_iterations, eps, hf)\n",
    "        bfgs_results.append(result_bfgs)\n",
    "        newton_results.append(result_newton)\n",
    "        trust_region_results.append(result_trust_region)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_bfgs = pd.DataFrame(bfgs_results)\n",
    "df_newton = pd.DataFrame(newton_results)\n",
    "df_trust_region = pd.DataFrame(trust_region_results)\n",
    "\n",
    "# numeric_columns = [\"distance_to_optimum\", \"num_evaluations\", \"final_function_value\", \"final_gradient_norm\"]\n",
    "\n",
    "# bfgs_avg = df_bfgs[numeric_columns].mean()\n",
    "# newton_avg = df_newton[numeric_columns].mean()\n",
    "# trust_region_avg = df_trust_region[numeric_columns].mean()\n",
    "\n",
    "grad_norms_bfgs = df_bfgs[\"grad_norms\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b4b18",
   "metadata": {},
   "source": [
    "Runs Each Optimizer for All 12 Starting Points (x0s):\n",
    "\n",
    "Calls test_optimizer() separately for each x0 and collects results.\n",
    "Computes the Average Per Step:\n",
    "\n",
    "Since some optimizations may terminate early, we pad shorter sequences with their last value to ensure consistency.\n",
    "Computes the mean of all x0s at each iteration.\n",
    "Plots the Averages with a Log Scale on the y-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7615e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# List of functions and their gradients and Hessians\n",
    "functions = [f1, f2, f3, f4, f5]\n",
    "dfs = [df1, df2, df3, df4, df5]\n",
    "Hfs = [Hf1, Hf2, Hf3, Hf4, Hf5]\n",
    "\n",
    "# Define multiple starting points\n",
    "x0s = [np.array([-5, -5]), np.array([-5, 5]), np.array([5, -5]), np.array([5, 5]),\n",
    "       np.array([-2, -2]), np.array([-2, 2]), np.array([2, -2]), np.array([2, 2]),\n",
    "       np.array([-1, -1]), np.array([-1, 1]), np.array([1, -1]), np.array([1, 1])]\n",
    "\n",
    "# Maximum iterations allowed for each optimization\n",
    "MAX_ITER_STOP = 50\n",
    "\n",
    "\n",
    "# Function to compute the gradient norm at each point\n",
    "def compute_gradient_norm(xs, df):\n",
    "    return [np.linalg.norm(df(x)) for x in xs]\n",
    "\n",
    "# Function to run the optimizer and return results\n",
    "def test_optimizer(optimizer, f, df, x0, max_iterations, epsilon, Hf=None):\n",
    "    if Hf is not None:\n",
    "        xs, grad_norms = optimizer(f, df, Hf, x0, max_iterations, epsilon)\n",
    "    else:\n",
    "        xs, grad_norms = optimizer(f, df, x0, max_iterations, epsilon)\n",
    "\n",
    "    xs = xs[:MAX_ITER_STOP]\n",
    "    grad_norms = grad_norms[:MAX_ITER_STOP]\n",
    "\n",
    "    gradients = compute_gradient_norm(xs, df)\n",
    "\n",
    "    return gradients, len(xs)\n",
    "\n",
    "# Function to compute the average across all starting points\n",
    "def average_across_starts(optimizer, f, df, Hf, max_iterations, epsilon):\n",
    "    all_grad_norms = []\n",
    "    \n",
    "    for x0 in x0s:\n",
    "        if optimizer == scipy_bfgs:\n",
    "            grad_norms, _ = test_optimizer(optimizer, f, df, x0, max_iterations, epsilon)\n",
    "        else:\n",
    "            grad_norms, _ = test_optimizer(optimizer, f, df, x0, max_iterations, epsilon, Hf)\n",
    "        \n",
    "        all_grad_norms.append(grad_norms)\n",
    "\n",
    "    # Pad shorter sequences with last value\n",
    "    # ligner ikke der bare bliver sat 0 p√•\n",
    "    max_len = max(len(d) for d in all_grad_norms)\n",
    "    all_grad_norms = [g + [0] * (max_len - len(g)) for g in all_grad_norms]\n",
    "    print(all_grad_norms)\n",
    "    avg_grad_norms = np.mean(all_grad_norms, axis=0)\n",
    "\n",
    "    return  avg_grad_norms, max_len\n",
    "\n",
    "\n",
    "# Function to plot the average gradient norm\n",
    "def plot_gradient_norm():\n",
    "    optimizers = [scipy_bfgs, scipy_newton, scipy_trust_region]\n",
    "    optimizer_names = ['BFGS', 'Newton', 'Trust Region']\n",
    "    max_iterations = 1000\n",
    "    epsilon = 1e-10\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        for f, df, Hf in zip(functions, dfs, Hfs):\n",
    "            avg_grad_norms, n_iterations = average_across_starts(optimizer, f, df, Hf, max_iterations, epsilon)\n",
    "            plt.plot(range(n_iterations), avg_grad_norms, label=f\"{optimizer.__name__} - {f.__name__}\")\n",
    "\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Average Gradient Norm')\n",
    "    plt.title('Optimizer Performance Comparison (Gradient Norm)')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def plot_specific_gradient_norm(optimizer):\n",
    "    max_iterations = 1000\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    \n",
    "    for f, df, Hf in zip(functions, dfs, Hfs):\n",
    "        match f.__name__:\n",
    "            case \"f1\":\n",
    "                f_name = \"The Ellipsoid function\"\n",
    "            case \"f2\":\n",
    "                f_name = \"The Rosenbrock Banana Function\"\n",
    "            case \"f3\":\n",
    "                f_name = \"The Log-Ellipsoid Function\"\n",
    "            case \"f4\":\n",
    "                f_name = \"The Attractive-Sector Function\"\n",
    "            case \"f5\":\n",
    "                f_name = \"The Sum of Different Powers Function\"\n",
    "        avg_grad_norms, n_iterations = average_across_starts(optimizer, f, df, Hf, max_iterations, epsilon)\n",
    "        plt.plot(range(n_iterations), avg_grad_norms, label=f\"{f_name}\")\n",
    "\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Average Gradient Norm')\n",
    "    plt.title(f'{optimizer.__name__} - Graident norms for each function, averaged over starting points')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_gradient_norm()\n",
    "plot_specific_gradient_norm(scipy_bfgs)\n",
    "plot_specific_gradient_norm(scipy_newton)\n",
    "plot_specific_gradient_norm(scipy_trust_region)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
