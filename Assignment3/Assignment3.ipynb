{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from case_studies import *\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, df, x, pk, alpha_init, c1, rho):\n",
    "    \n",
    "    alpha = alpha_init\n",
    "    while f(x + alpha * pk) > f(x) + c1 * alpha * np.dot(df(x), pk):\n",
    "        alpha *= rho\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max iter afhænger nok af newton som kalder CG, måske skal det hedde noget andet\n",
    "def conjugate_gradients(Q, g, eps, max_iter=1000):\n",
    "    x = 0\n",
    "    grad = g\n",
    "    p = -grad\n",
    "    xs = [x]\n",
    "    for i in range(max_iter):\n",
    "        # skal måske ik være minus\n",
    "        alpha_k = -(np.dot(p.T, grad) / np.dot(p.T, np.dot(Q, p)))\n",
    "        \n",
    "        x = x + alpha_k * p\n",
    "        \n",
    "        grad = np.dot(Q, x) + g\n",
    "        \n",
    "        if np.linalg.norm(grad) < eps:\n",
    "            break\n",
    "\n",
    "        p = -grad + (np.dot(grad.T, np.dot(Q, p)) / np.dot(p.T, np.dot(Q, p))) * p\n",
    "        \n",
    "        xs.append(x)\n",
    "    return x, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_newton(x, df, hf, c1, rho, max_iter=1000, tol=1e-6):\n",
    "    xs = [x]\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        if np.linalg.norm(df(x)) < tol:\n",
    "            break \n",
    "        \n",
    "        n_k = 0.5 * min(0.5, np.sqrt(np.linalg.norm(df(x))))\n",
    "        eps = n_k * np.linalg.norm(df(x))\n",
    "        p = conjugate_gradients(hf(x), df(x), eps)\n",
    "        alpha = backtracking_line_search(x, p, 1.0, c1, rho)\n",
    "        x = x + alpha * p\n",
    "        xs.append(x)\n",
    "    return x, xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f, df, optimizer, x0, x_opt, Hf):\n",
    "    start_time = time.time()\n",
    "\n",
    "    if Hf is not None:\n",
    "        x_final, xs = optimizer(f, df, Hf, x0)\n",
    "    else:\n",
    "        x_final, xs = optimizer(f, df, x0)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    num_iterations = len(xs)\n",
    "    final_solution_point = x_final\n",
    "    dist_to_optimum = np.linalg.norm(x_final-x_opt)\n",
    "    final_fun_value = f(xs[-1])\n",
    "    duration = end_time-start_time\n",
    "    \n",
    "    grad_norms = [np.linalg.norm(df(x)) for x in xs]\n",
    "\n",
    "    return (num_iterations, duration, final_fun_value, final_solution_point, dist_to_optimum, grad_norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = np.random.randn(2) \n",
    "# benchmark_results = []\n",
    "\n",
    "# for f, df, Hf, fname in zip(fs, dfs, Hfs, fnames):\n",
    "#     x_optimal = x_opt(fname, len(x0))\n",
    "\n",
    "#     sd_result = benchmark(f, df, steepest_descent, x0, x_optimal, Hf=None)\n",
    "#     benchmark_results.append((fname, \"Steepest Descent\") + sd_result)\n",
    "\n",
    "# for f, df, Hf, fname in zip(fs, dfs, Hfs, fnames):    \n",
    "#     x_optimal = x_opt(fname, len(x0))\n",
    "    \n",
    "#     nm_result = benchmark(f, df, newtons_method, x0, x_optimal, Hf)\n",
    "#     benchmark_results.append((fname, \"Newton's Method\") + nm_result)\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# columns = [\"Function\", \"Optimizer\", \"Iterations\", \"Time\", \"Final Function Value\", \"Final Solution Point\", \"Distance to Optimum\", \"Gradient Norms\"]\n",
    "# df_results = pd.DataFrame(benchmark_results, columns=columns)\n",
    "\n",
    "# df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure(figsize=(12, 8))\n",
    "# for i in range(5):\n",
    "    # plt.plot(range(len(df_results.iloc[i,7][:100])), df_results.iloc[i,7][:100], label=f\"{df_results.iloc[i,0]}\")\n",
    "# plt.xlabel('Steps')\n",
    "# plt.ylabel('Gradient Norm')\n",
    "# plt.title(\"Steepest Descent - Graident norms for each function\")\n",
    "# plt.yscale(\"log\")\n",
    "# plt.legend()\n",
    "# plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
