{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#if this fails, you need to put the case_studies.py file in the same folder\n",
    "#from case_studies import *\n",
    "from case_studies import *\n",
    "from scipy.optimize import minimize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking_line_search(f, grad_f, x, p, alpha_init=1.0, c1=1e-4, rho=0.9):\n",
    "    \"\"\"\n",
    "    Performs backtracking line search to find a suitable step size.\n",
    "    \n",
    "    Parameters:\n",
    "        f: Function to minimize.\n",
    "        grad_f: Gradient of f.\n",
    "        x: Current point.\n",
    "        p: Search direction.\n",
    "        alpha_init: Initial step size.\n",
    "        c1: Armijo condition parameter.\n",
    "        rho: Reduction factor for step size.\n",
    "    \n",
    "    Returns:\n",
    "        alpha: Suitable step size.\n",
    "    \"\"\"\n",
    "    alpha = alpha_init\n",
    "    while f(x + alpha * p) > f(x) + c1 * alpha * np.dot(grad_f(x), p):\n",
    "        alpha *= rho\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steepest Descent\n",
    "\n",
    "def steepest_descent(f, grad_f, x0, c1=1e-4, rho=0.9, tol=1e-6, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Implements the Steepest Descent Algorithm with Backtracking Line Search.\n",
    "    \n",
    "    Parameters:\n",
    "        f: Function to minimize.\n",
    "        grad_f: Gradient of f.\n",
    "        x0: Initial point.\n",
    "        c1: Armijo condition parameter.\n",
    "        rho: Reduction factor for step size.\n",
    "        tol: Tolerance for stopping condition.\n",
    "        max_iters: Maximum number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "        x: Estimated minimum.\n",
    "        history: List of iterates for analysis.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    beta = 1.0  # Initial beta value\n",
    "    history = [x0]\n",
    "    \n",
    "    for k in range(max_iters):\n",
    "        p = -grad_f(x)  # Steepest descent direction\n",
    "        if np.linalg.norm(p) < tol:\n",
    "            break  # Stopping condition\n",
    "        \n",
    "        alpha = backtracking_line_search(f, grad_f, x, p, beta, c1, rho)\n",
    "        x = x + alpha * p\n",
    "        beta = alpha / rho  # Update beta\n",
    "        history.append(x)\n",
    "    \n",
    "    return x, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Newton's Algorithm\n",
    "\n",
    "def newtons_method(f, grad_f, hessian_f, x0, c1=1e-4, rho=0.9, tol=1e-6, max_iters=1000):\n",
    "    \"\"\"\n",
    "    Implements Newton's Method with a modified Hessian when necessary.\n",
    "    \n",
    "    Parameters:\n",
    "        f: Function to minimize.\n",
    "        grad_f: Gradient of f.\n",
    "        hessian_f: Hessian (second derivative) of f.\n",
    "        x0: Initial point.\n",
    "        c1: Armijo condition parameter.\n",
    "        rho: Reduction factor for step size.\n",
    "        tol: Tolerance for stopping condition.\n",
    "        max_iters: Maximum number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "        x: Estimated minimum.\n",
    "        history: List of iterates for analysis.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    history = [x0]\n",
    "    \n",
    "    for k in range(max_iters):\n",
    "        grad = grad_f(x)\n",
    "        hessian = hessian_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break  # Stopping condition\n",
    "        \n",
    "        if np.all(np.linalg.eigvals(hessian) > 0):\n",
    "            p = -np.linalg.solve(hessian, grad)  # Newton's direction\n",
    "        else:\n",
    "            eigvals, eigvecs = np.linalg.eigh(hessian)\n",
    "            H = sum((1 / abs(eigval)) * np.outer(eigvec, eigvec) for eigval, eigvec in zip(eigvals, eigvecs))\n",
    "            p = -H @ grad  # Modified direction\n",
    "        \n",
    "        alpha = backtracking_line_search(f, grad_f, x, p, 1.0, c1, rho)\n",
    "        x = x + alpha * p\n",
    "        history.append(x)\n",
    "    \n",
    "    return x, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark Results:\n",
      "Function | SD Iter | SD f(x) | SD Error | SD Time | Newton Iter | Newton f(x) | Newton Error | Newton Time\n",
      "   f1    |  1001   | 0.0049  |  0.0697  | 0.2784  |      2      |  0.0000  |   0.0000   |  0.0040 \n",
      "   f2    |  1001   | 0.0045  |  0.1492  | 0.0380  |     14      |  0.0000  |   0.0000   |  0.0022 \n",
      "   f3    |  1001   | 3.5520  |  0.0577  | 0.3085  |      6      |  0.0000  |   0.0000   |  0.0052 \n",
      "   f4    |   113   | 0.0000  |  0.0000  | 0.0516  |      8      |  0.0000  |   0.0000   |  0.0042 \n",
      "   f5    |  1001   | 0.0000  |  0.0110  | 0.1958  |     12      |  0.0000  |   0.0061   |  0.0051 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of functions, gradients, and Hessians\n",
    "functions = [f1, f2, f3, f4, f5]\n",
    "dfs = [df1, df2, df3, df4, df5]\n",
    "Hfs = [Hf1, Hf2, Hf3, Hf4, Hf5]\n",
    "function_names = [\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"]\n",
    "\n",
    "# Function to run benchmark\n",
    "def benchmark_algorithm(algorithm, f, grad_f, hessian_f, x0, x_opt, algo_name):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if algo_name == \"Newton\":\n",
    "        x_min, history = newtons_method(f, grad_f, hessian_f, x0)\n",
    "    else:\n",
    "        x_min, history = steepest_descent(f, grad_f, x0)\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    num_iterations = len(history)\n",
    "    final_value = f(x_min)\n",
    "    error = np.linalg.norm(x_min - x_opt)\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return num_iterations, final_value, error, time_taken\n",
    "\n",
    "# Run benchmarks\n",
    "results = []\n",
    "\n",
    "for i in range(len(functions)):\n",
    "    f, df, Hf = functions[i], dfs[i], Hfs[i]\n",
    "    x0 = np.random.randn(2)  # Initialize x0 randomly in 2D space   ### THIS IS FOR ROSENBROCK FUNCTION AS ITS MAXXED AT 2D\n",
    "    x_opt_val = x_opt(function_names[i], len(x0))\n",
    "\n",
    "    # Benchmark Steepest Descent\n",
    "    sd_metrics = benchmark_algorithm(steepest_descent, f, df, Hf, x0, x_opt_val, \"Steepest Descent\")\n",
    "\n",
    "    # Benchmark Newton's Method\n",
    "    newton_metrics = benchmark_algorithm(newtons_method, f, df, Hf, x0, x_opt_val, \"Newton\")\n",
    "\n",
    "    # Store results\n",
    "    results.append((function_names[i], *sd_metrics, *newton_metrics))\n",
    "\n",
    "# Print results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "print(\"Function | SD Iter | SD f(x) | SD Error | SD Time | Newton Iter | Newton f(x) | Newton Error | Newton Time\")\n",
    "for r in results:\n",
    "    print(f\"{r[0]:^8} | {r[1]:^7} | {r[2]:^7.4f} | {r[3]:^8.4f} | {r[4]:^7.4f} | {r[5]:^11} | {r[6]:^8.4f} | {r[7]:^10.4f} | {r[8]:^8.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
